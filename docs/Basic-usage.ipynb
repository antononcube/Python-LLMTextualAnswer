{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLMTextualAnswer Python package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates the usage of the Python package [\"LLMTextualAnswer\"](https://pypi.org/project/LLMTextualAnswer/), [AAp1], which provides function(s) for finding sub-strings in texts that appear to be answers to given questions according to Large Language Models (LLMs). The package implementation closely follows the implementations of the Raku package [\"ML::FindTextualAnswer\"](https://raku.land/zef:antononcube/ML::FindTextualAnswer), [AAp1], and Wolfram Language function [`LLMTextualAnswer`](https://resources.wolframcloud.com/FunctionRepository/resources/LLMTextualAnswer/), [AAf1]. Both, in turn, were inspired by the Wolfram Language function [`FindTextualAnswer`](https://reference.wolfram.com/language/ref/FindTextualAnswer.html), [WRIf1, JL1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark:** Currently, LLMs are utilized via the Python [\"LangChain\"](https://www.langchain.com) packages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark:** One of the primary motivations for implementing this package is to provide the fundamental functionality of extracting parameter values from (domain specific) texts needed for the implementation for the Python version of the Raku package [\"ML::NLPTemplateEngine\"](https://raku.land/zef:antononcube/ML::NLPTemplateEngine), [AAp3]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To install the package from [PyPI.org](https://pypi.org/) use the shell command:\n",
    "\n",
    "```shell\n",
    "python3 -m install LLMFindTextualAnswer\n",
    "```\n",
    "\n",
    "To install the package from the GitHub repository use the shell command:\n",
    "\n",
    "```shell\n",
    "python3 -m pip install git+https://github.com/antononcube/Python-LLMTextualAnswer.git#egg=LLMTextualAnswer\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load packages and define LLM access objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LLMTextualAnswer import *\n",
    "from langchain_ollama import ChatOllama\n",
    "import os\n",
    "\n",
    "llm = ChatOllama(model=os.getenv(\"OLLAMA_MODEL\", \"gemma3:4b\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of finding textual answers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Where is Titicaca?': 'The Andes, on the border of Bolivia Peru'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "Lake Titicaca is a large, deep lake in the Andes \n",
    "on the border of Bolivia and Peru. By volume of water and by surface \n",
    "area, it is the largest lake in South America\n",
    "\"\"\"\n",
    "\n",
    "llm_textual_answer(text, \"Where is Titicaca?\", llm=llm, form = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default `llm_textual_answer` tries to give short answers. If the option \"request\" is `None` then depending on the number of questions the request is one those phrases:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \"give the shortest answer of the question:\"\n",
    "- \"list the shortest answers of the questions:\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above the full query given to LLM is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Given the text \"Lake Titicaca is a large, deep lake in the Andes on the border of Bolivia and Peru. By volume of water and by surface area, it is the largest lake in South America\" give the shortest answer of the question:   \n",
    "> Where is Titicaca?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we get a longer answer by changing the value of \"request\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Where is Titicaca?': 'The Andes, on the border of Bolivia Peru'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_textual_answer(text, \"Where is Titicaca?\", request = \"answer the question:\", llm = llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark:** The function `find-textual-answer` is inspired by the Mathematica function [`FindTextualAnswer`](https://reference.wolfram.com/language/ref/FindTextualAnswer.html), [WRIf1]; see [JL1] for details. Unfortunately, at this time implementing the full signature of `FindTextualAnswer` with LLM-provider APIs is not easy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the text: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "862"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textCap = \"\"\"\n",
    "Born and raised in the Austrian Empire, Tesla studied engineering and physics in the 1870s without receiving a degree,\n",
    "gaining practical experience in the early 1880s working in telephony and at Continental Edison in the new electric power industry.\n",
    "\n",
    "In 1884 he emigrated to the United States, where he became a naturalized citizen.\n",
    "He worked for a short time at the Edison Machine Works in New York City before he struck out on his own.\n",
    "With the help of partners to finance and market his ideas,\n",
    "Tesla set up laboratories and companies in New York to develop a range of electrical and mechanical devices.\n",
    "His alternating current (AC) induction motor and related polyphase AC patents, licensed by Westinghouse Electric in 1888,\n",
    "earned him a considerable amount of money and became the cornerstone of the polyphase system which that company eventually marketed.\n",
    "\"\"\"\n",
    "\n",
    "len(textCap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we ask a single question and request 3 answers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Where lived?': 'Austrian Empire, United States, New York City'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_textual_answer(textCap, 'Where lived?', n = 3, llm = llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a rerun without number of answers argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Where lived?': 'Austrian Empire, United States'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_textual_answer(textCap, 'Where lived?', llm = llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If several questions are given to the function `llm_textual_answer` then all questions are spliced with the given text into one query (that is sent to LLM.)\n",
    "\n",
    "For example, consider the following text and questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'Make a classifier with the method RandomForest over the data dfTitanic; show precision and accuracy.';\n",
    "\n",
    "questions = [\n",
    "        'What is the dataset?',\n",
    "        'What is the method?',\n",
    "        'Which metrics to show?'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the query send to the LLM is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Given the text: \"Make a classifier with the method RandomForest over the data dfTitanic; show precision and accuracy.\"\n",
    "> list the shortest answers of the questions:   \n",
    "> 1) What is the dataset?   \n",
    "> 2) What is the method?    \n",
    "> 3) Which metrics to show?   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The answers are assumed to be given in the same order as the questions, each answer in a separated line. Hence, by splitting the LLM result into lines we get the answers corresponding to the questions.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark:** For some LLMs, if the questions are missing question marks, it is likely that the result may have a completion as a first line followed by the answers. In that situation the answers are not parsed and a warning message is given."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of requesting answers of multiple questions and specifying that the result should be a dictionary: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the dataset? : dfTitanic\n",
      "What is the method? : RandomForest\n",
      "Which metrics to show? : Precision, accuracy\n"
     ]
    }
   ],
   "source": [
    "res = llm_textual_answer(query, questions, llm = llm, form = dict)\n",
    "\n",
    "for (k,v) in res.items():\n",
    "    print(f\"{k} : {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mermaid diagram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following flowchart corresponds to the ***conceptual*** steps in the package function `llm_textual_answer`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "graph LR\n",
    "\tUI[/\"1) Natural language text<br>2) Questions\"/]\n",
    "\tTO[/\"Answers\"/]\n",
    "\tWR[[Web request]]\n",
    "\tOpenAI{{OpenAI}}\n",
    "\tGemini{{Gemini}}\n",
    "\tOllama{{Ollama}}\n",
    "\tPJ[Parse JSON]\n",
    "\tQ{Return<br>dictionary?}\n",
    "\tMSTC[Compose query]\n",
    "\tMURL[[Make URL]]\n",
    "\tTTC[Process]\n",
    "\tQAK{Auth key<br>supplied?}\n",
    "\tEAK[[\"Try to find<br>API key<br>in %*ENV\"]]\n",
    "\tQEAF{Auth key<br>found?}\n",
    "\tNAK[/Cannot find auth key/]\n",
    "\tUI --> QAK\n",
    "\tQAK --> |yes|MSTC\n",
    "\tQAK --> |no|EAK\n",
    "\tEAK --> QEAF\n",
    "\tMSTC --> TTC\n",
    "\tQEAF --> |no|NAK\n",
    "\tQEAF --> |yes|TTC\n",
    "\tTTC -.-> MURL -.-> WR -.-> TTC\n",
    "\tWR -.-> |URL|OpenAI \n",
    "\tWR -.-> |URL|Gemini \n",
    "\tWR -.-> |URL|Ollama \n",
    "\tOpenAI -.-> |JSON|WR\n",
    "\tGemini -.-> |JSON|WR\n",
    "\tOllama -.-> |JSON|WR\n",
    "\tTTC --> Q \n",
    "\tQ --> |yes|PJ\n",
    "\tQ --> |no|TO\n",
    "\tPJ --> TO\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Articles\n",
    "\n",
    "[JL1] Jérôme Louradour, [\"New in the Wolfram Language: FindTextualAnswer\"](https://blog.wolfram.com/2018/02/15/new-in-the-wolfram-language-findtextualanswer), (2018), [blog.wolfram.com](https://blog.wolfram.com/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[AAf1] Anton Antonov, [LLMTextualAnswer](https://resources.wolframcloud.com/FunctionRepository/resources/LLMTextualAnswer/), (2024), [Wolfram Function Repository](https://resources.wolframcloud.com/FunctionRepository).\n",
    "\n",
    "[WRIf1] Wolfram Research, Inc., [FindTextualAnswer](https://reference.wolfram.com/language/ref/FindTextualAnswer.html), [Wolfram Language function](https://reference.wolfram.com/language/), (2018), (updated 2020)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[AAp1] Anton Antonov, [LLMTextualAnswer, Python package](https://github.com/antononcube/Python-LLMTextualAnswer), (2026), [GitHub/antononcube](https://github.com/antononcube).\n",
    "\n",
    "[AAp2] Anton Antonov, [ML::FindTextualAnswer, Raku package](https://github.com/antononcube/Raku-ML-FindTextualAnswer), (2023-2025), [GitHub/antononcube](https://github.com/antononcube).\n",
    "\n",
    "[AAp3] Anton Antonov, [ML::NLPTemplateEngine, Raku package](https://github.com/antononcube/Raku-ML-NLPTemplateEngine), (2023-2025), [GitHub/antononcube](https://github.com/antononcube)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SciPyCentric",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "file_extension": ".raku",
   "mimetype": "text/x-raku",
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
